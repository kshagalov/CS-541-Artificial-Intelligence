{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NX91lyH2a-N7",
    "outputId": "359d8b1b-a249-4018-d7bc-41b83a443d47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'edm'...\n",
      "remote: Enumerating objects: 54, done.\u001b[K\n",
      "remote: Counting objects: 100% (34/34), done.\u001b[K\n",
      "remote: Compressing objects: 100% (21/21), done.\u001b[K\n",
      "remote: Total 54 (delta 13), reused 13 (delta 13), pack-reused 20 (from 1)\u001b[K\n",
      "Receiving objects: 100% (54/54), 2.13 MiB | 5.46 MiB/s, done.\n",
      "Resolving deltas: 100% (13/13), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/NVlabs/edm.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example file provided from Github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BV-U9tuWtxvI",
    "outputId": "a3d96023-efff-48e5-9c1b-a3f26f647aa5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading network from \"https://nvlabs-fi-cdn.nvidia.com/edm/pretrained/edm-cifar10-32x32-cond-vp.pkl\"...\n",
      "Downloading https://nvlabs-fi-cdn.nvidia.com/edm/pretrained/edm-cifar10-32x32-cond-vp.pkl ... done\n",
      "Generating 64 images...\n",
      "100% 18/18 [00:02<00:00,  7.87step/s]\n",
      "Saving image grid to \"cifar10-32x32.png\"...\n",
      "Done.\n",
      "Loading network from \"https://nvlabs-fi-cdn.nvidia.com/edm/pretrained/edm-ffhq-64x64-uncond-vp.pkl\"...\n",
      "Generating 64 images...\n",
      "100% 40/40 [00:08<00:00,  4.50step/s]\n",
      "Saving image grid to \"ffhq-64x64.png\"...\n",
      "Done.\n",
      "Loading network from \"https://nvlabs-fi-cdn.nvidia.com/edm/pretrained/edm-afhqv2-64x64-uncond-vp.pkl\"...\n",
      "Generating 64 images...\n",
      "100% 40/40 [00:08<00:00,  4.50step/s]\n",
      "Saving image grid to \"afhqv2-64x64.png\"...\n",
      "Done.\n",
      "Loading network from \"https://nvlabs-fi-cdn.nvidia.com/edm/pretrained/edm-imagenet-64x64-cond-adm.pkl\"...\n",
      "Downloading https://nvlabs-fi-cdn.nvidia.com/edm/pretrained/edm-imagenet-64x64-cond-adm.pkl ... done\n",
      "Generating 64 images...\n",
      "100% 256/256 [01:45<00:00,  2.43step/s]\n",
      "Saving image grid to \"imagenet-64x64.png\"...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "!python edm/example.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the pre-trained models to produce results and Calculate their FID values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UUCM6aHpIP-L",
    "outputId": "a97c284c-de2b-4074-bad9-acf1ab7ced7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset reference statistics from \"https://nvlabs-fi-cdn.nvidia.com/edm/fid-refs/cifar10-32x32.npz\"...\n",
      "Downloading https://nvlabs-fi-cdn.nvidia.com/edm/fid-refs/cifar10-32x32.npz ... done\n",
      "Loading Inception-v3 model...\n",
      "Downloading https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/metrics/inception-2015-12-05.pkl ... done\n",
      "Loading images from \"fid-tmp\"...\n",
      "[rank0]:[W1209 21:38:10.525395502 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n",
      "Calculating statistics for 50000 images...\n",
      "100% 782/782 [00:31<00:00, 24.58batch/s]\n",
      "Calculating FID...\n",
      "1.84866\n"
     ]
    }
   ],
   "source": [
    "#run the CIFAR-10 model and calculate FID \n",
    "!torchrun --standalone --nproc_per_node=1 edm/generate.py --outdir=fid-tmp --seeds=0-49999 --subdirs \\\n",
    "    --network=https://nvlabs-fi-cdn.nvidia.com/edm/pretrained/edm-cifar10-32x32-cond-vp.pkl\n",
    "!torchrun --standalone --nproc_per_node=1 edm/fid.py calc --images=fid-tmp \\\n",
    "    --ref=https://nvlabs-fi-cdn.nvidia.com/edm/fid-refs/cifar10-32x32.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wZcRn4FkBvHS",
    "outputId": "5f864121-8c91-4370-ebe2-62888af5ee25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading network from \"https://nvlabs-fi-cdn.nvidia.com/edm/pretrained/edm-ffhq-64x64-uncond-vp.pkl\"...\n",
      "Downloading https://nvlabs-fi-cdn.nvidia.com/edm/pretrained/edm-ffhq-64x64-uncond-vp.pkl ... done\n",
      "[rank0]:[W1210 19:38:06.255998052 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n",
      "Generating 50000 images to \"fid-tmp\"...\n",
      "  2% 19/782 [01:18<51:48,  4.07s/batch]W1210 19:39:27.234000 54374 torch/distributed/elastic/agent/server/api.py:704] Received Signals.SIGINT death signal, shutting down workers\n",
      "W1210 19:39:27.234000 54374 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 54396 closing signal SIGINT\n",
      "  2% 19/782 [01:20<54:03,  4.25s/batch]\n",
      "\n",
      "Aborted!\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/torchrun\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 919, in main\n",
      "    run(args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 910, in run\n",
      "    elastic_launch(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 260, in launch_agent\n",
      "    result = agent.run()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/metrics/api.py\", line 137, in wrapper\n",
      "    result = f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/agent/server/api.py\", line 696, in run\n",
      "    result = self._invoke_run(role)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/agent/server/api.py\", line 855, in _invoke_run\n",
      "    time.sleep(monitor_interval)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/api.py\", line 84, in _terminate_process_handler\n",
      "    raise SignalException(f\"Process {os.getpid()} got signal: {sigval}\", sigval=sigval)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/api.py\", line 84, in _terminate_process_handler\n",
      "    raise SignalException(f\"Process {os.getpid()} got signal: {sigval}\", sigval=sigval)\n",
      "torch.distributed.elastic.multiprocessing.api.SignalException: Process 54374 got signal: 2\n",
      "Loading dataset reference statistics from \"https://nvlabs-fi-cdn.nvidia.com/edm/fid-refs/ffhq-64x64.npz\"...\n",
      "Downloading https://nvlabs-fi-cdn.nvidia.com/edm/fid-refs/ffhq-64x64.npz ... done\n",
      "Loading Inception-v3 model...\n",
      "Loading images from \"fid-tmp\"...\n",
      "[rank0]:[W1210 19:39:37.992084929 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n",
      "Calculating statistics for 50000 images...\n",
      "  1% 6/782 [00:06<14:56,  1.15s/batch]\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \"/content/edm/fid.py\", line 163, in <module>\n",
      "[rank0]:     main()\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1157, in __call__\n",
      "[rank0]:     return self.main(*args, **kwargs)\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1078, in main\n",
      "[rank0]:     rv = self.invoke(ctx)\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1688, in invoke\n",
      "[rank0]:     return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1434, in invoke\n",
      "[rank0]:     return ctx.invoke(self.callback, **ctx.params)\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 783, in invoke\n",
      "[rank0]:     return __callback(*args, **kwargs)\n",
      "[rank0]:   File \"/content/edm/fid.py\", line 131, in calc\n",
      "[rank0]:     mu, sigma = calculate_inception_stats(image_path=image_path, num_expected=num_expected, seed=seed, max_batch_size=batch)\n",
      "[rank0]:   File \"/content/edm/fid.py\", line 62, in calculate_inception_stats\n",
      "[rank0]:     for images, _labels in tqdm.tqdm(data_loader, unit='batch', disable=(dist.get_rank() != 0)):\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/tqdm/std.py\", line 1181, in __iter__\n",
      "[rank0]:     for obj in iterable:\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 701, in __next__\n",
      "[rank0]:     data = self._next_data()\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1445, in _next_data\n",
      "[rank0]:     return self._process_data(data)\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1491, in _process_data\n",
      "[rank0]:     data.reraise()\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils.py\", line 715, in reraise\n",
      "[rank0]:     raise exception\n",
      "[rank0]: AssertionError: Caught AssertionError in DataLoader worker process 0.\n",
      "[rank0]: Original Traceback (most recent call last):\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n",
      "[rank0]:     data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n",
      "[rank0]:     data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n",
      "[rank0]:     data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "[rank0]:   File \"/content/edm/training/dataset.py\", line 98, in __getitem__\n",
      "[rank0]:     assert list(image.shape) == self.image_shape\n",
      "[rank0]: AssertionError\n",
      "\n",
      "E1210 19:39:44.848000 54809 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 54827) of binary: /usr/bin/python3\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/torchrun\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 919, in main\n",
      "    run(args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 910, in run\n",
      "    elastic_launch(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 269, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "edm/fid.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2024-12-10_19:39:44\n",
      "  host      : 159ac9015b3f\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 54827)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "#run the FFHQ model and calculate FID for FFHQ\n",
    "!torchrun --standalone --nproc_per_node=1 edm/generate.py --outdir=fid-tmp --seeds=0-49999 --subdirs \\\n",
    "    --network=https://nvlabs-fi-cdn.nvidia.com/edm/pretrained/edm-ffhq-64x64-uncond-vp.pkl\n",
    "!torchrun --standalone --nproc_per_node=1 edm/fid.py calc --images=fid-tmp \\\n",
    "    --ref=https://nvlabs-fi-cdn.nvidia.com/edm/fid-refs/ffhq-64x64.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YKvcXrfEB4Z8",
    "outputId": "ecebc5a6-cb4e-40cd-e077-0acd3a1af6b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading network from \"https://nvlabs-fi-cdn.nvidia.com/edm/pretrained/edm-afhqv2-64x64-uncond-vp.pkl\"...\n",
      "Downloading https://nvlabs-fi-cdn.nvidia.com/edm/pretrained/edm-afhqv2-64x64-uncond-vp.pkl ... done\n",
      "[rank0]:[W1210 19:41:11.941263560 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n",
      "Generating 50000 images to \"fid-tmp\"...\n",
      "  3% 23/782 [01:34<51:32,  4.07s/batch]W1210 19:42:49.718000 55384 torch/distributed/elastic/agent/server/api.py:704] Received Signals.SIGINT death signal, shutting down workers\n",
      "W1210 19:42:49.718000 55384 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 55402 closing signal SIGINT\n",
      "  3% 23/782 [01:37<53:37,  4.24s/batch]\n",
      "\n",
      "Aborted!\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/torchrun\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 919, in main\n",
      "    run(args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 910, in run\n",
      "    elastic_launch(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 260, in launch_agent\n",
      "    result = agent.run()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/metrics/api.py\", line 137, in wrapper\n",
      "    result = f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/agent/server/api.py\", line 696, in run\n",
      "    result = self._invoke_run(role)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/agent/server/api.py\", line 855, in _invoke_run\n",
      "    time.sleep(monitor_interval)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/api.py\", line 84, in _terminate_process_handler\n",
      "    raise SignalException(f\"Process {os.getpid()} got signal: {sigval}\", sigval=sigval)\n",
      "  File \"/usr/lib/python3.10/enum.py\", line 774, in __format__\n",
      "    if self._member_type_ is object or str_overridden:\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/api.py\", line 84, in _terminate_process_handler\n",
      "    raise SignalException(f\"Process {os.getpid()} got signal: {sigval}\", sigval=sigval)\n",
      "torch.distributed.elastic.multiprocessing.api.SignalException: Process 55384 got signal: 2\n",
      "Loading dataset reference statistics from \"https://nvlabs-fi-cdn.nvidia.com/edm/fid-refs/afhqv2-64x64.npz\"...\n",
      "Downloading https://nvlabs-fi-cdn.nvidia.com/edm/fid-refs/afhqv2-64x64.npz ... done\n",
      "Loading Inception-v3 model...\n",
      "Loading images from \"fid-tmp\"...\n",
      "[rank0]:[W1210 19:42:59.471299432 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n",
      "Calculating statistics for 50000 images...\n",
      "  1% 7/782 [00:06<12:49,  1.01batch/s]\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \"/content/edm/fid.py\", line 163, in <module>\n",
      "[rank0]:     main()\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1157, in __call__\n",
      "[rank0]:     return self.main(*args, **kwargs)\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1078, in main\n",
      "[rank0]:     rv = self.invoke(ctx)\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1688, in invoke\n",
      "[rank0]:     return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1434, in invoke\n",
      "[rank0]:     return ctx.invoke(self.callback, **ctx.params)\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 783, in invoke\n",
      "[rank0]:     return __callback(*args, **kwargs)\n",
      "[rank0]:   File \"/content/edm/fid.py\", line 131, in calc\n",
      "[rank0]:     mu, sigma = calculate_inception_stats(image_path=image_path, num_expected=num_expected, seed=seed, max_batch_size=batch)\n",
      "[rank0]:   File \"/content/edm/fid.py\", line 62, in calculate_inception_stats\n",
      "[rank0]:     for images, _labels in tqdm.tqdm(data_loader, unit='batch', disable=(dist.get_rank() != 0)):\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/tqdm/std.py\", line 1181, in __iter__\n",
      "[rank0]:     for obj in iterable:\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 701, in __next__\n",
      "[rank0]:     data = self._next_data()\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1465, in _next_data\n",
      "[rank0]:     return self._process_data(data)\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1491, in _process_data\n",
      "[rank0]:     data.reraise()\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils.py\", line 715, in reraise\n",
      "[rank0]:     raise exception\n",
      "[rank0]: AssertionError: Caught AssertionError in DataLoader worker process 1.\n",
      "[rank0]: Original Traceback (most recent call last):\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n",
      "[rank0]:     data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n",
      "[rank0]:     data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n",
      "[rank0]:     data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "[rank0]:   File \"/content/edm/training/dataset.py\", line 98, in __getitem__\n",
      "[rank0]:     assert list(image.shape) == self.image_shape\n",
      "[rank0]: AssertionError\n",
      "\n",
      "E1210 19:43:07.344000 55869 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 55895) of binary: /usr/bin/python3\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/torchrun\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 919, in main\n",
      "    run(args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 910, in run\n",
      "    elastic_launch(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 269, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "edm/fid.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2024-12-10_19:43:07\n",
      "  host      : 159ac9015b3f\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 55895)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "#run the AFHQv2 model and calculate FID\n",
    "!torchrun --standalone --nproc_per_node=1 edm/generate.py --outdir=fid-tmp --seeds=0-49999 --subdirs \\\n",
    "    --network=https://nvlabs-fi-cdn.nvidia.com/edm/pretrained/edm-afhqv2-64x64-uncond-vp.pkl\n",
    "!torchrun --standalone --nproc_per_node=1 edm/fid.py calc --images=fid-tmp \\\n",
    "    --ref=https://nvlabs-fi-cdn.nvidia.com/edm/fid-refs/afhqv2-64x64.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EvDYx6fDB63U"
   },
   "outputs": [],
   "source": [
    "#run the ImageNet64 model and calculate FID\n",
    "!torchrun --standalone --nproc_per_node=1 edm/generate.py --outdir=fid-tmp --seeds=0-49999 --subdirs \\\n",
    "    --network=https://nvlabs-fi-cdn.nvidia.com/edm/pretrained/edm-imagenet-64x64-cond-adm.pkl\n",
    "!torchrun --standalone --nproc_per_node=1 edm/fid.py calc --images=fid-tmp \\\n",
    "    --ref=https://nvlabs-fi-cdn.nvidia.com/edm/fid-refs/imagenet-64x64.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OQdE9sRhxsgl",
    "outputId": "a47616f1-b086-48e7-bb0e-37e48c535364"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: idx2numpy in /usr/local/lib/python3.10/dist-packages (1.2.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from idx2numpy) (1.26.4)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from idx2numpy) (1.16.0)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement dnnlib (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for dnnlib\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install idx2numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following code is for creating the dataset properly from the MNIST zip file. Skip execution when dataset folder is provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yLswoQKhZWkF"
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import PIL.Image\n",
    "import zipfile\n",
    "import idx2numpy\n",
    "import os\n",
    "import json\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7_iarnUTxu-a",
    "outputId": "add318fe-3c2c-47b4-e8d6-d6ce356df721"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Images Shape: (60000, 28, 28)\n",
      "Train Labels Shape: (60000,)\n",
      "Downsampled Images Shape: (600, 28, 28)\n",
      "Downsampled Labels Shape: (600,)\n"
     ]
    }
   ],
   "source": [
    "zip_file = '/content/archive.zip' \n",
    "extract_folder = '/content/dataset/'\n",
    "with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_folder)\n",
    "\n",
    "train_images_path = os.path.join(extract_folder, 'train-images-idx3-ubyte/train-images-idx3-ubyte')\n",
    "train_labels_path = os.path.join(extract_folder, 'train-labels-idx1-ubyte/train-labels-idx1-ubyte')\n",
    "\n",
    "train_images = idx2numpy.convert_from_file(train_images_path)\n",
    "train_labels = idx2numpy.convert_from_file(train_labels_path)\n",
    "\n",
    "print(\"Train Images Shape:\", train_images.shape)\n",
    "print(\"Train Labels Shape:\", train_labels.shape)\n",
    "\n",
    "sample_size = int(0.01 * train_images.shape[0])  # 1% of the dataset size\n",
    "indices = np.random.choice(train_images.shape[0], sample_size, replace=False) \n",
    "downsampled_images = train_images[indices]\n",
    "downsampled_labels = train_labels[indices]\n",
    "\n",
    "print(\"Downsampled Images Shape:\", downsampled_images.shape)\n",
    "print(\"Downsampled Labels Shape:\", downsampled_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "byM2Aebex4JB"
   },
   "outputs": [],
   "source": [
    "def save_images_and_create_json(images, labels, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    dataset = {\"labels\": []}\n",
    "\n",
    "    for idx, (image, label) in enumerate(zip(images, labels)):\n",
    "        label_dir = os.path.join(output_dir, str(label))\n",
    "        if not os.path.exists(label_dir):\n",
    "            os.makedirs(label_dir)\n",
    "        img = Image.fromarray(image)\n",
    "        img_file = f\"img{idx:08d}.png\"\n",
    "        img.save(os.path.join(label_dir, img_file))\n",
    "        dataset[\"labels\"].append([f\"{label}/{img_file}\", int(label)])\n",
    "\n",
    "    with open(os.path.join(output_dir, 'dataset.json'), 'w') as f:\n",
    "        json.dump(dataset, f, indent=2)\n",
    "\n",
    "save_images_and_create_json(downsampled_images, downsampled_labels, 'mnist_dataset_downsampled')  #saving downsampled data\n",
    "save_images_and_create_json(train_images, train_labels, 'mnist_dataset')    #saving full dataset for calculating reference statistics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BONUS Creating and training our own new model based on the MNIST dataset (downsampled). Skip execution if model file is supplied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dLSIGZTfeHIF",
    "outputId": "fa193a79-61c7-49bb-b779-726b42b7e6ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training options:\n",
      "{\n",
      "  \"dataset_kwargs\": {\n",
      "    \"class_name\": \"training.dataset.ImageFolderDataset\",\n",
      "    \"path\": \"mnist_dataset_downsampled\",\n",
      "    \"use_labels\": true,\n",
      "    \"xflip\": false,\n",
      "    \"cache\": true,\n",
      "    \"resolution\": 28,\n",
      "    \"max_size\": 600\n",
      "  },\n",
      "  \"data_loader_kwargs\": {\n",
      "    \"pin_memory\": true,\n",
      "    \"num_workers\": 1,\n",
      "    \"prefetch_factor\": 2\n",
      "  },\n",
      "  \"network_kwargs\": {\n",
      "    \"model_type\": \"SongUNet\",\n",
      "    \"embedding_type\": \"positional\",\n",
      "    \"encoder_type\": \"standard\",\n",
      "    \"decoder_type\": \"standard\",\n",
      "    \"channel_mult_noise\": 1,\n",
      "    \"resample_filter\": [\n",
      "      1,\n",
      "      1\n",
      "    ],\n",
      "    \"model_channels\": 128,\n",
      "    \"channel_mult\": [\n",
      "      2,\n",
      "      2,\n",
      "      2\n",
      "    ],\n",
      "    \"class_name\": \"training.networks.EDMPrecond\",\n",
      "    \"augment_dim\": 9,\n",
      "    \"dropout\": 0.13,\n",
      "    \"use_fp16\": true\n",
      "  },\n",
      "  \"loss_kwargs\": {\n",
      "    \"class_name\": \"training.loss.EDMLoss\"\n",
      "  },\n",
      "  \"optimizer_kwargs\": {\n",
      "    \"class_name\": \"torch.optim.Adam\",\n",
      "    \"lr\": 0.001,\n",
      "    \"betas\": [\n",
      "      0.9,\n",
      "      0.999\n",
      "    ],\n",
      "    \"eps\": 1e-08\n",
      "  },\n",
      "  \"augment_kwargs\": {\n",
      "    \"class_name\": \"training.augment.AugmentPipe\",\n",
      "    \"p\": 0.12,\n",
      "    \"xflip\": 100000000.0,\n",
      "    \"yflip\": 1,\n",
      "    \"scale\": 1,\n",
      "    \"rotate_frac\": 1,\n",
      "    \"aniso\": 1,\n",
      "    \"translate_frac\": 1\n",
      "  },\n",
      "  \"total_kimg\": 6000,\n",
      "  \"ema_halflife_kimg\": 500,\n",
      "  \"batch_size\": 256,\n",
      "  \"batch_gpu\": null,\n",
      "  \"loss_scaling\": 1.0,\n",
      "  \"cudnn_benchmark\": true,\n",
      "  \"kimg_per_tick\": 50,\n",
      "  \"snapshot_ticks\": 50,\n",
      "  \"state_dump_ticks\": 500,\n",
      "  \"seed\": 646300070,\n",
      "  \"run_dir\": \"training-runs/00000-mnist_dataset_downsampled-cond-ddpmpp-edm-gpus1-batch256-fp16\"\n",
      "}\n",
      "\n",
      "Output directory:        training-runs/00000-mnist_dataset_downsampled-cond-ddpmpp-edm-gpus1-batch256-fp16\n",
      "Dataset path:            mnist_dataset_downsampled\n",
      "Class-conditional:       True\n",
      "Network architecture:    ddpmpp\n",
      "Preconditioning & loss:  edm\n",
      "Number of GPUs:          1\n",
      "Batch size:              256\n",
      "Mixed-precision:         True\n",
      "\n",
      "Creating output directory...\n",
      "Loading dataset...\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/sampler.py:76: UserWarning: `data_source` argument is not used and will be removed in 2.2.0.You may still have custom implementation that utilizes it.\n",
      "  warnings.warn(\n",
      "Constructing network...\n",
      "\n",
      "EDMPrecond                Parameters  Buffers  Output shape        Datatype\n",
      "---                       ---         ---      ---                 ---     \n",
      "model.map_noise           -           -        [256, 128]          float32 \n",
      "model.map_label           1408        -        [256, 128]          float32 \n",
      "model.map_layer0          66048       -        [256, 512]          float32 \n",
      "model.map_layer1          262656      -        [256, 512]          float32 \n",
      "model.enc.28x28_conv      1280        -        [256, 128, 28, 28]  float16 \n",
      "model.enc.28x28_block0    1050368     -        [256, 256, 28, 28]  float16 \n",
      "model.enc.28x28_block1    1312512     -        [256, 256, 28, 28]  float16 \n",
      "model.enc.28x28_block2    1312512     -        [256, 256, 28, 28]  float16 \n",
      "model.enc.28x28_block3    1312512     -        [256, 256, 28, 28]  float16 \n",
      "model.enc.14x14_down      1378304     8        [256, 256, 14, 14]  float16 \n",
      "model.enc.14x14_block0    1312512     -        [256, 256, 14, 14]  float16 \n",
      "model.enc.14x14_block1    1312512     -        [256, 256, 14, 14]  float16 \n",
      "model.enc.14x14_block2    1312512     -        [256, 256, 14, 14]  float16 \n",
      "model.enc.14x14_block3    1312512     -        [256, 256, 14, 14]  float16 \n",
      "model.enc.7x7_down        1378304     8        [256, 256, 7, 7]    float16 \n",
      "model.enc.7x7_block0      1312512     -        [256, 256, 7, 7]    float16 \n",
      "model.enc.7x7_block1      1312512     -        [256, 256, 7, 7]    float16 \n",
      "model.enc.7x7_block2      1312512     -        [256, 256, 7, 7]    float16 \n",
      "model.enc.7x7_block3      1312512     -        [256, 256, 7, 7]    float16 \n",
      "model.dec.7x7_in0         1576192     -        [256, 256, 7, 7]    float16 \n",
      "model.dec.7x7_in1         1312512     -        [256, 256, 7, 7]    float16 \n",
      "model.dec.7x7_block0      2034176     -        [256, 256, 7, 7]    float16 \n",
      "model.dec.7x7_block1      2034176     -        [256, 256, 7, 7]    float16 \n",
      "model.dec.7x7_block2      2034176     -        [256, 256, 7, 7]    float16 \n",
      "model.dec.7x7_block3      2034176     -        [256, 256, 7, 7]    float16 \n",
      "model.dec.7x7_block4      2034176     -        [256, 256, 7, 7]    float16 \n",
      "model.dec.14x14_up        1378304     8        [256, 256, 14, 14]  float16 \n",
      "model.dec.14x14_block0    2034176     -        [256, 256, 14, 14]  float16 \n",
      "model.dec.14x14_block1    2034176     -        [256, 256, 14, 14]  float16 \n",
      "model.dec.14x14_block2    2034176     -        [256, 256, 14, 14]  float16 \n",
      "model.dec.14x14_block3    2034176     -        [256, 256, 14, 14]  float16 \n",
      "model.dec.14x14_block4    2034176     -        [256, 256, 14, 14]  float16 \n",
      "model.dec.28x28_up        1378304     8        [256, 256, 28, 28]  float16 \n",
      "model.dec.28x28_block0    2034176     -        [256, 256, 28, 28]  float16 \n",
      "model.dec.28x28_block1    2034176     -        [256, 256, 28, 28]  float16 \n",
      "model.dec.28x28_block2    2034176     -        [256, 256, 28, 28]  float16 \n",
      "model.dec.28x28_block3    2034176     -        [256, 256, 28, 28]  float16 \n",
      "model.dec.28x28_block4    1706240     -        [256, 256, 28, 28]  float16 \n",
      "model.dec.28x28_aux_norm  512         -        [256, 256, 28, 28]  float16 \n",
      "model.dec.28x28_aux_conv  2305        -        [256, 1, 28, 28]    float16 \n",
      "model                     1152        -        [256, 1, 28, 28]    float16 \n",
      "<top-level>               -           -        [256, 1, 28, 28]    float32 \n",
      "---                       ---         ---      ---                 ---     \n",
      "Total                     54409985    32       -                   -       \n",
      "\n",
      "Setting up optimizer...\n",
      "Training for 6000 kimg...\n",
      "\n",
      "tick 0     kimg 0.3       time 10s          sec/tick 2.6     sec/kimg 10.23   maintenance 7.4    cpumem 1.85   gpumem 12.31  reserved 13.00 \n",
      "tick 1     kimg 50.4      time 1m 08s       sec/tick 57.0    sec/kimg 1.14    maintenance 0.7    cpumem 2.24   gpumem 12.08  reserved 13.24 \n",
      "tick 2     kimg 100.6     time 2m 01s       sec/tick 53.5    sec/kimg 1.07    maintenance 0.0    cpumem 2.24   gpumem 12.08  reserved 13.22 \n",
      "tick 3     kimg 150.8     time 2m 54s       sec/tick 52.4    sec/kimg 1.04    maintenance 0.0    cpumem 2.24   gpumem 12.08  reserved 13.22 \n",
      "tick 4     kimg 201.0     time 3m 46s       sec/tick 52.6    sec/kimg 1.05    maintenance 0.0    cpumem 2.24   gpumem 12.08  reserved 13.22 \n",
      "tick 5     kimg 251.1     time 4m 39s       sec/tick 52.4    sec/kimg 1.05    maintenance 0.0    cpumem 2.24   gpumem 12.08  reserved 13.22 \n",
      "tick 6     kimg 301.3     time 5m 31s       sec/tick 52.1    sec/kimg 1.04    maintenance 0.0    cpumem 2.24   gpumem 12.08  reserved 13.24 \n",
      "tick 7     kimg 351.5     time 6m 23s       sec/tick 52.0    sec/kimg 1.04    maintenance 0.0    cpumem 2.24   gpumem 12.08  reserved 13.22 \n",
      "tick 8     kimg 401.7     time 7m 15s       sec/tick 52.1    sec/kimg 1.04    maintenance 0.0    cpumem 2.24   gpumem 12.08  reserved 13.24 \n",
      "tick 9     kimg 451.8     time 8m 07s       sec/tick 52.1    sec/kimg 1.04    maintenance 0.0    cpumem 2.24   gpumem 12.08  reserved 13.22 \n",
      "tick 10    kimg 502.0     time 8m 59s       sec/tick 51.9    sec/kimg 1.03    maintenance 0.0    cpumem 2.24   gpumem 12.08  reserved 13.22 \n",
      "tick 11    kimg 552.2     time 9m 51s       sec/tick 51.6    sec/kimg 1.03    maintenance 0.0    cpumem 2.24   gpumem 12.08  reserved 13.22 \n",
      "tick 12    kimg 602.4     time 10m 42s      sec/tick 51.7    sec/kimg 1.03    maintenance 0.0    cpumem 2.24   gpumem 12.08  reserved 13.22 \n",
      "tick 13    kimg 652.5     time 11m 34s      sec/tick 51.7    sec/kimg 1.03    maintenance 0.0    cpumem 2.24   gpumem 12.08  reserved 13.22 \n",
      "tick 14    kimg 702.7     time 12m 25s      sec/tick 51.5    sec/kimg 1.03    maintenance 0.0    cpumem 2.24   gpumem 12.08  reserved 13.22 \n",
      "tick 15    kimg 752.9     time 13m 17s      sec/tick 52.0    sec/kimg 1.04    maintenance 0.0    cpumem 2.24   gpumem 12.08  reserved 13.22 \n",
      "tick 16    kimg 803.1     time 14m 09s      sec/tick 51.5    sec/kimg 1.03    maintenance 0.0    cpumem 2.24   gpumem 12.08  reserved 13.22 \n",
      "tick 17    kimg 853.2     time 15m 01s      sec/tick 51.7    sec/kimg 1.03    maintenance 0.0    cpumem 2.24   gpumem 12.08  reserved 13.22 \n",
      "tick 18    kimg 903.4     time 15m 53s      sec/tick 51.9    sec/kimg 1.03    maintenance 0.0    cpumem 2.24   gpumem 12.08  reserved 13.22 \n",
      "tick 19    kimg 953.6     time 16m 44s      sec/tick 51.8    sec/kimg 1.03    maintenance 0.0    cpumem 2.24   gpumem 12.08  reserved 13.22 \n",
      "tick 20    kimg 1003.8    time 17m 36s      sec/tick 52.0    sec/kimg 1.04    maintenance 0.0    cpumem 2.24   gpumem 12.08  reserved 13.24 \n",
      "tick 21    kimg 1054.0    time 18m 28s      sec/tick 51.8    sec/kimg 1.03    maintenance 0.0    cpumem 2.24   gpumem 12.08  reserved 13.22 \n",
      "tick 22    kimg 1104.1    time 19m 20s      sec/tick 51.5    sec/kimg 1.03    maintenance 0.0    cpumem 2.24   gpumem 12.08  reserved 13.22 \n",
      "tick 23    kimg 1154.3    time 20m 11s      sec/tick 51.5    sec/kimg 1.03    maintenance 0.0    cpumem 2.24   gpumem 12.08  reserved 13.22 \n",
      "tick 24    kimg 1204.5    time 21m 03s      sec/tick 52.0    sec/kimg 1.04    maintenance 0.0    cpumem 2.24   gpumem 12.08  reserved 13.24 \n",
      "tick 25    kimg 1254.7    time 21m 55s      sec/tick 51.8    sec/kimg 1.03    maintenance 0.0    cpumem 2.24   gpumem 12.08  reserved 13.22 \n",
      "tick 26    kimg 1304.8    time 22m 47s      sec/tick 51.7    sec/kimg 1.03    maintenance 0.0    cpumem 2.24   gpumem 12.08  reserved 13.24 \n",
      "tick 27    kimg 1355.0    time 23m 38s      sec/tick 51.5    sec/kimg 1.03    maintenance 0.0    cpumem 2.24   gpumem 12.08  reserved 13.22 \n",
      "tick 28    kimg 1405.2    time 24m 30s      sec/tick 51.5    sec/kimg 1.03    maintenance 0.0    cpumem 2.24   gpumem 12.08  reserved 13.24 \n",
      "tick 29    kimg 1455.4    time 25m 21s      sec/tick 51.7    sec/kimg 1.03    maintenance 0.0    cpumem 2.24   gpumem 12.08  reserved 13.24 \n",
      "tick 30    kimg 1505.5    time 26m 13s      sec/tick 51.5    sec/kimg 1.03    maintenance 0.0    cpumem 2.24   gpumem 12.08  reserved 13.24 \n",
      "tick 31    kimg 1555.7    time 27m 04s      sec/tick 51.4    sec/kimg 1.02    maintenance 0.0    cpumem 2.24   gpumem 12.08  reserved 13.24 \n",
      "tick 32    kimg 1605.9    time 27m 56s      sec/tick 51.3    sec/kimg 1.02    maintenance 0.0    cpumem 2.24   gpumem 12.08  reserved 13.22 \n",
      "tick 33    kimg 1656.1    time 28m 47s      sec/tick 51.4    sec/kimg 1.02    maintenance 0.0    cpumem 2.24   gpumem 12.08  reserved 13.22 \n",
      "tick 34    kimg 1706.2    time 29m 38s      sec/tick 51.5    sec/kimg 1.03    maintenance 0.0    cpumem 2.24   gpumem 12.08  reserved 13.24 \n",
      "tick 35    kimg 1756.4    time 30m 30s      sec/tick 51.5    sec/kimg 1.03    maintenance 0.0    cpumem 2.24   gpumem 12.08  reserved 13.24 \n",
      "tick 36    kimg 1806.6    time 31m 21s      sec/tick 51.4    sec/kimg 1.02    maintenance 0.0    cpumem 2.24   gpumem 12.08  reserved 13.22 \n",
      "tick 37    kimg 1856.8    time 32m 13s      sec/tick 51.6    sec/kimg 1.03    maintenance 0.0    cpumem 2.24   gpumem 12.08  reserved 13.22 \n",
      "tick 38    kimg 1906.9    time 33m 04s      sec/tick 51.5    sec/kimg 1.03    maintenance 0.0    cpumem 2.24   gpumem 12.08  reserved 13.22 \n",
      "tick 39    kimg 1957.1    time 33m 56s      sec/tick 51.5    sec/kimg 1.03    maintenance 0.0    cpumem 2.24   gpumem 12.08  reserved 13.24 \n",
      "tick 40    kimg 2007.3    time 34m 47s      sec/tick 51.5    sec/kimg 1.03    maintenance 0.0    cpumem 2.24   gpumem 12.08  reserved 13.24 \n",
      "tick 41    kimg 2057.5    time 35m 39s      sec/tick 51.4    sec/kimg 1.02    maintenance 0.0    cpumem 2.24   gpumem 12.08  reserved 13.24 \n",
      "tick 42    kimg 2107.6    time 36m 30s      sec/tick 51.6    sec/kimg 1.03    maintenance 0.0    cpumem 2.24   gpumem 12.08  reserved 13.24 \n",
      "tick 43    kimg 2157.8    time 37m 22s      sec/tick 51.2    sec/kimg 1.02    maintenance 0.0    cpumem 2.24   gpumem 12.08  reserved 13.22 \n",
      "tick 44    kimg 2208.0    time 38m 13s      sec/tick 51.4    sec/kimg 1.02    maintenance 0.0    cpumem 2.24   gpumem 12.08  reserved 13.24 \n",
      "tick 45    kimg 2258.2    time 39m 04s      sec/tick 51.4    sec/kimg 1.02    maintenance 0.0    cpumem 2.24   gpumem 12.08  reserved 13.22 \n",
      "tick 46    kimg 2308.4    time 39m 56s      sec/tick 51.4    sec/kimg 1.02    maintenance 0.0    cpumem 2.24   gpumem 12.08  reserved 13.22 \n",
      "tick 47    kimg 2358.5    time 40m 47s      sec/tick 51.2    sec/kimg 1.02    maintenance 0.0    cpumem 2.24   gpumem 12.08  reserved 13.22 \n",
      "tick 48    kimg 2408.7    time 41m 38s      sec/tick 51.3    sec/kimg 1.02    maintenance 0.0    cpumem 2.24   gpumem 12.08  reserved 13.22 \n",
      "tick 49    kimg 2458.9    time 42m 30s      sec/tick 51.5    sec/kimg 1.03    maintenance 0.0    cpumem 2.24   gpumem 12.08  reserved 13.24 \n",
      "tick 50    kimg 2509.1    time 43m 21s      sec/tick 51.5    sec/kimg 1.03    maintenance 0.0    cpumem 2.24   gpumem 12.08  reserved 13.24 \n",
      "tick 51    kimg 2559.2    time 44m 13s      sec/tick 51.2    sec/kimg 1.02    maintenance 0.5    cpumem 2.23   gpumem 12.08  reserved 13.22 \n",
      "tick 52    kimg 2609.4    time 45m 04s      sec/tick 51.2    sec/kimg 1.02    maintenance 0.0    cpumem 2.23   gpumem 12.08  reserved 13.22 \n",
      "tick 53    kimg 2659.6    time 45m 56s      sec/tick 51.4    sec/kimg 1.02    maintenance 0.0    cpumem 2.23   gpumem 12.08  reserved 13.22 \n",
      "tick 54    kimg 2709.8    time 46m 47s      sec/tick 51.4    sec/kimg 1.02    maintenance 0.0    cpumem 2.23   gpumem 12.08  reserved 13.22 \n",
      "tick 55    kimg 2759.9    time 47m 38s      sec/tick 51.4    sec/kimg 1.02    maintenance 0.0    cpumem 2.23   gpumem 12.08  reserved 13.22 \n",
      "tick 56    kimg 2810.1    time 48m 30s      sec/tick 51.3    sec/kimg 1.02    maintenance 0.0    cpumem 2.23   gpumem 12.08  reserved 13.22 \n",
      "tick 57    kimg 2860.3    time 49m 21s      sec/tick 51.3    sec/kimg 1.02    maintenance 0.0    cpumem 2.23   gpumem 12.08  reserved 13.22 \n",
      "tick 58    kimg 2910.5    time 50m 13s      sec/tick 51.4    sec/kimg 1.02    maintenance 0.0    cpumem 2.23   gpumem 12.08  reserved 13.22 \n",
      "tick 59    kimg 2960.6    time 51m 04s      sec/tick 51.5    sec/kimg 1.03    maintenance 0.0    cpumem 2.23   gpumem 12.08  reserved 13.24 \n",
      "tick 60    kimg 3010.8    time 51m 55s      sec/tick 51.5    sec/kimg 1.03    maintenance 0.0    cpumem 2.23   gpumem 12.08  reserved 13.24 \n",
      "tick 61    kimg 3061.0    time 52m 47s      sec/tick 51.5    sec/kimg 1.03    maintenance 0.0    cpumem 2.23   gpumem 12.08  reserved 13.22 \n",
      "tick 62    kimg 3111.2    time 53m 38s      sec/tick 51.3    sec/kimg 1.02    maintenance 0.0    cpumem 2.23   gpumem 12.08  reserved 13.22 \n",
      "tick 63    kimg 3161.3    time 54m 30s      sec/tick 51.3    sec/kimg 1.02    maintenance 0.0    cpumem 2.23   gpumem 12.08  reserved 13.22 \n",
      "tick 64    kimg 3211.5    time 55m 21s      sec/tick 51.2    sec/kimg 1.02    maintenance 0.0    cpumem 2.23   gpumem 12.08  reserved 13.22 \n",
      "tick 65    kimg 3261.7    time 56m 12s      sec/tick 51.2    sec/kimg 1.02    maintenance 0.0    cpumem 2.23   gpumem 12.08  reserved 13.22 \n",
      "tick 66    kimg 3311.9    time 57m 03s      sec/tick 51.4    sec/kimg 1.02    maintenance 0.0    cpumem 2.23   gpumem 12.08  reserved 13.22 \n",
      "tick 67    kimg 3362.0    time 57m 55s      sec/tick 51.4    sec/kimg 1.02    maintenance 0.0    cpumem 2.23   gpumem 12.08  reserved 13.22 \n",
      "tick 68    kimg 3412.2    time 58m 46s      sec/tick 51.3    sec/kimg 1.02    maintenance 0.0    cpumem 2.23   gpumem 12.08  reserved 13.24 \n",
      "tick 69    kimg 3462.4    time 59m 38s      sec/tick 51.4    sec/kimg 1.03    maintenance 0.0    cpumem 2.23   gpumem 12.08  reserved 13.24 \n",
      "tick 70    kimg 3512.6    time 1h 00m 29s   sec/tick 51.3    sec/kimg 1.02    maintenance 0.0    cpumem 2.23   gpumem 12.08  reserved 13.24 \n",
      "tick 71    kimg 3562.8    time 1h 01m 20s   sec/tick 51.5    sec/kimg 1.03    maintenance 0.0    cpumem 2.23   gpumem 12.08  reserved 13.22 \n",
      "tick 72    kimg 3612.9    time 1h 02m 12s   sec/tick 51.2    sec/kimg 1.02    maintenance 0.0    cpumem 2.23   gpumem 12.08  reserved 13.22 \n",
      "tick 73    kimg 3663.1    time 1h 03m 03s   sec/tick 51.2    sec/kimg 1.02    maintenance 0.0    cpumem 2.23   gpumem 12.08  reserved 13.22 \n",
      "tick 74    kimg 3713.3    time 1h 03m 54s   sec/tick 51.2    sec/kimg 1.02    maintenance 0.0    cpumem 2.23   gpumem 12.08  reserved 13.24 \n",
      "tick 75    kimg 3763.5    time 1h 04m 45s   sec/tick 51.2    sec/kimg 1.02    maintenance 0.0    cpumem 2.23   gpumem 12.08  reserved 13.24 \n",
      "tick 76    kimg 3813.6    time 1h 05m 37s   sec/tick 51.5    sec/kimg 1.03    maintenance 0.0    cpumem 2.23   gpumem 12.08  reserved 13.22 \n",
      "tick 77    kimg 3863.8    time 1h 06m 28s   sec/tick 51.3    sec/kimg 1.02    maintenance 0.0    cpumem 2.23   gpumem 12.08  reserved 13.22 \n",
      "tick 78    kimg 3914.0    time 1h 07m 19s   sec/tick 51.2    sec/kimg 1.02    maintenance 0.0    cpumem 2.23   gpumem 12.08  reserved 13.22 \n",
      "tick 79    kimg 3964.2    time 1h 08m 10s   sec/tick 51.2    sec/kimg 1.02    maintenance 0.0    cpumem 2.23   gpumem 12.08  reserved 13.22 \n",
      "tick 80    kimg 4014.3    time 1h 09m 01s   sec/tick 51.2    sec/kimg 1.02    maintenance 0.0    cpumem 2.23   gpumem 12.08  reserved 13.22 \n",
      "tick 81    kimg 4064.5    time 1h 09m 53s   sec/tick 51.3    sec/kimg 1.02    maintenance 0.0    cpumem 2.23   gpumem 12.08  reserved 13.24 \n",
      "tick 82    kimg 4114.7    time 1h 10m 44s   sec/tick 51.2    sec/kimg 1.02    maintenance 0.0    cpumem 2.23   gpumem 12.08  reserved 13.24 \n",
      "tick 83    kimg 4164.9    time 1h 11m 35s   sec/tick 51.1    sec/kimg 1.02    maintenance 0.0    cpumem 2.23   gpumem 12.08  reserved 13.22 \n",
      "tick 84    kimg 4215.0    time 1h 12m 26s   sec/tick 51.2    sec/kimg 1.02    maintenance 0.0    cpumem 2.23   gpumem 12.08  reserved 13.24 \n",
      "tick 85    kimg 4265.2    time 1h 13m 18s   sec/tick 51.5    sec/kimg 1.03    maintenance 0.0    cpumem 2.23   gpumem 12.08  reserved 13.24 \n",
      "tick 86    kimg 4315.4    time 1h 14m 09s   sec/tick 51.4    sec/kimg 1.02    maintenance 0.0    cpumem 2.23   gpumem 12.08  reserved 13.24 \n",
      "tick 87    kimg 4365.6    time 1h 15m 01s   sec/tick 51.4    sec/kimg 1.02    maintenance 0.0    cpumem 2.23   gpumem 12.08  reserved 13.24 \n",
      "tick 88    kimg 4415.7    time 1h 15m 52s   sec/tick 51.2    sec/kimg 1.02    maintenance 0.0    cpumem 2.23   gpumem 12.08  reserved 13.24 \n",
      "tick 89    kimg 4465.9    time 1h 16m 43s   sec/tick 51.2    sec/kimg 1.02    maintenance 0.0    cpumem 2.23   gpumem 12.08  reserved 13.24 \n",
      "tick 90    kimg 4516.1    time 1h 17m 34s   sec/tick 51.1    sec/kimg 1.02    maintenance 0.0    cpumem 2.23   gpumem 12.08  reserved 13.22 \n",
      "tick 91    kimg 4566.3    time 1h 18m 25s   sec/tick 51.2    sec/kimg 1.02    maintenance 0.0    cpumem 2.23   gpumem 12.08  reserved 13.22 \n",
      "tick 92    kimg 4616.4    time 1h 19m 17s   sec/tick 51.4    sec/kimg 1.02    maintenance 0.0    cpumem 2.23   gpumem 12.08  reserved 13.22 \n",
      "tick 93    kimg 4666.6    time 1h 20m 08s   sec/tick 51.2    sec/kimg 1.02    maintenance 0.0    cpumem 2.23   gpumem 12.08  reserved 13.22 \n",
      "tick 94    kimg 4716.8    time 1h 20m 59s   sec/tick 51.2    sec/kimg 1.02    maintenance 0.0    cpumem 2.23   gpumem 12.08  reserved 13.22 \n",
      "tick 95    kimg 4767.0    time 1h 21m 50s   sec/tick 51.1    sec/kimg 1.02    maintenance 0.0    cpumem 2.23   gpumem 12.08  reserved 13.22 \n",
      "tick 96    kimg 4817.2    time 1h 22m 41s   sec/tick 51.1    sec/kimg 1.02    maintenance 0.0    cpumem 2.23   gpumem 12.08  reserved 13.22 \n",
      "tick 97    kimg 4867.3    time 1h 23m 33s   sec/tick 51.4    sec/kimg 1.02    maintenance 0.0    cpumem 2.23   gpumem 12.08  reserved 13.22 \n",
      "tick 98    kimg 4917.5    time 1h 24m 24s   sec/tick 51.3    sec/kimg 1.02    maintenance 0.0    cpumem 2.23   gpumem 12.08  reserved 13.24 \n",
      "tick 99    kimg 4967.7    time 1h 25m 15s   sec/tick 51.1    sec/kimg 1.02    maintenance 0.0    cpumem 2.23   gpumem 12.08  reserved 13.24 \n",
      "tick 100   kimg 5017.9    time 1h 26m 06s   sec/tick 51.1    sec/kimg 1.02    maintenance 0.0    cpumem 2.23   gpumem 12.08  reserved 13.22 \n",
      "tick 101   kimg 5068.0    time 1h 26m 58s   sec/tick 51.0    sec/kimg 1.02    maintenance 0.4    cpumem 2.23   gpumem 12.08  reserved 13.22 \n",
      "tick 102   kimg 5118.2    time 1h 27m 49s   sec/tick 51.2    sec/kimg 1.02    maintenance 0.0    cpumem 2.23   gpumem 12.08  reserved 13.24 \n",
      "tick 103   kimg 5168.4    time 1h 28m 40s   sec/tick 51.1    sec/kimg 1.02    maintenance 0.0    cpumem 2.23   gpumem 12.08  reserved 13.24 \n",
      "tick 104   kimg 5218.6    time 1h 29m 31s   sec/tick 51.0    sec/kimg 1.02    maintenance 0.0    cpumem 2.23   gpumem 12.08  reserved 13.24 \n",
      "tick 105   kimg 5268.7    time 1h 30m 22s   sec/tick 51.1    sec/kimg 1.02    maintenance 0.0    cpumem 2.23   gpumem 12.08  reserved 13.24 \n",
      "tick 106   kimg 5318.9    time 1h 31m 13s   sec/tick 51.0    sec/kimg 1.02    maintenance 0.0    cpumem 2.23   gpumem 12.08  reserved 13.22 \n",
      "tick 107   kimg 5369.1    time 1h 32m 05s   sec/tick 51.5    sec/kimg 1.03    maintenance 0.0    cpumem 2.23   gpumem 12.08  reserved 13.24 \n",
      "tick 108   kimg 5419.3    time 1h 32m 56s   sec/tick 51.1    sec/kimg 1.02    maintenance 0.0    cpumem 2.23   gpumem 12.08  reserved 13.22 \n",
      "tick 109   kimg 5469.4    time 1h 33m 47s   sec/tick 51.0    sec/kimg 1.02    maintenance 0.0    cpumem 2.23   gpumem 12.08  reserved 13.22 \n",
      "tick 110   kimg 5519.6    time 1h 34m 38s   sec/tick 51.1    sec/kimg 1.02    maintenance 0.0    cpumem 2.23   gpumem 12.08  reserved 13.24 \n",
      "tick 111   kimg 5569.8    time 1h 35m 29s   sec/tick 51.3    sec/kimg 1.02    maintenance 0.0    cpumem 2.23   gpumem 12.08  reserved 13.24 \n",
      "tick 112   kimg 5620.0    time 1h 36m 20s   sec/tick 51.2    sec/kimg 1.02    maintenance 0.0    cpumem 2.23   gpumem 12.08  reserved 13.22 \n",
      "tick 113   kimg 5670.1    time 1h 37m 11s   sec/tick 51.1    sec/kimg 1.02    maintenance 0.0    cpumem 2.23   gpumem 12.08  reserved 13.22 \n",
      "tick 114   kimg 5720.3    time 1h 38m 02s   sec/tick 51.1    sec/kimg 1.02    maintenance 0.0    cpumem 2.23   gpumem 12.08  reserved 13.22 \n",
      "tick 115   kimg 5770.5    time 1h 38m 54s   sec/tick 51.3    sec/kimg 1.02    maintenance 0.0    cpumem 2.23   gpumem 12.08  reserved 13.22 \n",
      "tick 116   kimg 5820.7    time 1h 39m 45s   sec/tick 51.3    sec/kimg 1.02    maintenance 0.0    cpumem 2.23   gpumem 12.08  reserved 13.24 \n",
      "tick 117   kimg 5870.8    time 1h 40m 36s   sec/tick 51.3    sec/kimg 1.02    maintenance 0.0    cpumem 2.23   gpumem 12.08  reserved 13.24 \n",
      "tick 118   kimg 5921.0    time 1h 41m 27s   sec/tick 51.2    sec/kimg 1.02    maintenance 0.0    cpumem 2.23   gpumem 12.08  reserved 13.22 \n",
      "tick 119   kimg 5971.2    time 1h 42m 18s   sec/tick 51.1    sec/kimg 1.02    maintenance 0.0    cpumem 2.23   gpumem 12.08  reserved 13.24 \n",
      "tick 120   kimg 6000.1    time 1h 42m 48s   sec/tick 29.6    sec/kimg 1.02    maintenance 0.0    cpumem 2.23   gpumem 12.08  reserved 13.24 \n",
      "\n",
      "Exiting...\n"
     ]
    }
   ],
   "source": [
    "!torchrun --standalone --nproc_per_node=1 edm/train.py --outdir=training-runs \\\n",
    "    --data=mnist_dataset_downsampled --cond=1 --arch=ddpmpp --batch=256 --fp16=True \\\n",
    "    --duration=6 --lr=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qjpzEJJdUJnQ",
    "outputId": "b1167e8f-1aed-4856-d2ab-eac438064a20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading network from \"/content/training-runs/00000-mnist_dataset_downsampled-cond-ddpmpp-edm-gpus1-batch256-fp16/network-snapshot-006000.pkl\"...\n",
      "[rank0]:[W1210 18:25:51.270973276 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n",
      "Generating 64 images to \"out\"...\n",
      "100% 1/1 [00:01<00:00,  1.97s/batch]\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "#generate 64 images using our trained model\n",
    "!python edm/generate.py --outdir=out --steps=18 --solver=heun --disc=edm --schedule=linear --scaling=none \\\n",
    "    --network=/content/training-runs/00000-mnist_dataset_downsampled-cond-ddpmpp-edm-gpus1-batch256-fp16/network-snapshot-006000.pkl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gcm6bmaRVjsh",
    "outputId": "73a5203a-6bc2-425d-8f5d-6615d441ac8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Inception-v3 model...\n",
      "Loading images from \"mnist_dataset\"...\n",
      "[rank0]:[W1210 18:44:33.017858170 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n",
      "Calculating statistics for 60000 images...\n",
      "100% 938/938 [00:36<00:00, 25.36batch/s]\n",
      "Saving dataset reference statistics to \"fid-refs/MNISTdataset.npz\"...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "#calculating reference statistics\n",
    "!python edm/fid.py ref --data=mnist_dataset --dest=fid-refs/MNISTdataset.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VZ96nlObU-2X",
    "outputId": "a19e59e6-d16b-4a6b-e827-d412170cfa8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset reference statistics from \"fid-refs/MNISTdataset.npz\"...\n",
      "Loading Inception-v3 model...\n",
      "Loading images from \"fid-tmp\"...\n",
      "[rank0]:[W1210 18:46:31.758924847 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n",
      "Calculating statistics for 50000 images...\n",
      "100% 782/782 [00:31<00:00, 24.44batch/s]\n",
      "Calculating FID...\n",
      "8.34717\n"
     ]
    }
   ],
   "source": [
    "#calculating FID run 1\n",
    "!torchrun --standalone --nproc_per_node=1 edm/generate.py --outdir=fid-tmp --seeds=0-49999 --subdirs \\\n",
    "    --network=/content/training-runs/00000-mnist_dataset_downsampled-cond-ddpmpp-edm-gpus1-batch256-fp16/network-snapshot-006000.pkl\n",
    "!torchrun --standalone --nproc_per_node=1 edm/fid.py calc --images=fid-tmp \\\n",
    "    --ref=fid-refs/MNISTdataset.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wDRUdKbbVbOo",
    "outputId": "f579f43e-b669-4c08-9da1-81abdad04122"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading network from \"/content/training-runs/00000-mnist_dataset_downsampled-cond-ddpmpp-edm-gpus1-batch256-fp16/network-snapshot-006000.pkl\"...\n",
      "[rank0]:[W1210 18:49:37.318785599 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n",
      "Generating 50000 images to \"fid-tmp\"...\n",
      "100% 782/782 [15:17<00:00,  1.17s/batch]\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "#calculating FID run 2, change seeds to 50000-99999\n",
    "!torchrun --standalone --nproc_per_node=1 edm/generate.py --outdir=fid-tmp --seeds=50000-99999 --subdirs \\\n",
    "    --network=/content/training-runs/00000-mnist_dataset_downsampled-cond-ddpmpp-edm-gpus1-batch256-fp16/network-snapshot-006000.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XZLfDUNeVa6W",
    "outputId": "8ab98121-dead-41a6-da7f-61a626524c4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset reference statistics from \"fid-refs/MNISTdataset.npz\"...\n",
      "Loading Inception-v3 model...\n",
      "Loading images from \"fid-tmp\"...\n",
      "[rank0]:[W1210 19:06:30.284489281 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n",
      "Calculating statistics for 50000 images...\n",
      "100% 782/782 [00:32<00:00, 24.43batch/s]\n",
      "Calculating FID...\n",
      "8.32231\n"
     ]
    }
   ],
   "source": [
    "#calculate FID 2\n",
    "!torchrun --standalone --nproc_per_node=1 edm/fid.py calc --images=fid-tmp \\\n",
    "    --ref=fid-refs/MNISTdataset.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "77WD2r3aWiK8",
    "outputId": "5440521a-4c5d-4491-9511-ade4cddd90d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading network from \"/content/training-runs/00000-mnist_dataset_downsampled-cond-ddpmpp-edm-gpus1-batch256-fp16/network-snapshot-006000.pkl\"...\n",
      "[rank0]:[W1210 19:08:32.859410769 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n",
      "Generating 50000 images to \"fid-tmp\"...\n",
      "100% 782/782 [15:02<00:00,  1.15s/batch]\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "#calculating FID run 3, change seeds to 100000-149999\n",
    "!torchrun --standalone --nproc_per_node=1 edm/generate.py --outdir=fid-tmp --seeds=100000-149999 --subdirs \\\n",
    "    --network=/content/training-runs/00000-mnist_dataset_downsampled-cond-ddpmpp-edm-gpus1-batch256-fp16/network-snapshot-006000.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kpJfTwq9WigT",
    "outputId": "d08c4cb5-28c2-470f-be9c-95635b902ce9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset reference statistics from \"fid-refs/MNISTdataset.npz\"...\n",
      "Loading Inception-v3 model...\n",
      "Loading images from \"fid-tmp\"...\n",
      "[rank0]:[W1210 19:23:57.633528079 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n",
      "Calculating statistics for 50000 images...\n",
      "100% 782/782 [00:32<00:00, 24.35batch/s]\n",
      "Calculating FID...\n",
      "8.29461\n"
     ]
    }
   ],
   "source": [
    "#calculate FID 3\n",
    "!torchrun --standalone --nproc_per_node=1 edm/fid.py calc --images=fid-tmp \\\n",
    "    --ref=fid-refs/MNISTdataset.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XlfP966C-a9S",
    "outputId": "b0fada70-f65d-40a9-abc2-112db84a3f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading network from \"/content/training-runs/00000-mnist_dataset_downsampled-cond-ddpmpp-edm-gpus1-batch256-fp16/network-snapshot-006000.pkl\"...\n",
      "[rank0]:[W1210 19:25:34.801359426 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n",
      "Generating 50000 images to \"fid-tmp\"...\n",
      "  2% 15/782 [00:39<32:39,  2.55s/batch]W1210 19:26:16.573000 50877 torch/distributed/elastic/agent/server/api.py:704] Received Signals.SIGINT death signal, shutting down workers\n",
      "W1210 19:26:16.574000 50877 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 50899 closing signal SIGINT\n",
      "  2% 15/782 [00:41<35:21,  2.77s/batch]\n",
      "\n",
      "Aborted!\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/torchrun\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 919, in main\n",
      "    run(args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 910, in run\n",
      "    elastic_launch(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 260, in launch_agent\n",
      "    result = agent.run()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/metrics/api.py\", line 137, in wrapper\n",
      "    result = f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/agent/server/api.py\", line 696, in run\n",
      "    result = self._invoke_run(role)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/agent/server/api.py\", line 855, in _invoke_run\n",
      "    time.sleep(monitor_interval)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/api.py\", line 84, in _terminate_process_handler\n",
      "    raise SignalException(f\"Process {os.getpid()} got signal: {sigval}\", sigval=sigval)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/api.py\", line 69, in __init__\n",
      "    def __init__(self, msg: str, sigval: signal.Signals) -> None:\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/api.py\", line 84, in _terminate_process_handler\n",
      "    raise SignalException(f\"Process {os.getpid()} got signal: {sigval}\", sigval=sigval)\n",
      "torch.distributed.elastic.multiprocessing.api.SignalException: Process 50877 got signal: 2\n",
      "Loading dataset reference statistics from \"fid-refs/MNISTdataset.npz\"...\n",
      "Loading Inception-v3 model...\n",
      "Loading images from \"fid-tmp\"...\n",
      "[rank0]:[W1210 19:26:23.861225266 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n",
      "Calculating statistics for 50000 images...\n",
      "  0% 0/782 [00:00<?, ?batch/s]W1210 19:26:27.421000 51111 torch/distributed/elastic/agent/server/api.py:704] Received Signals.SIGINT death signal, shutting down workers\n",
      "W1210 19:26:27.421000 51111 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 51131 closing signal SIGINT\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  0% 0/782 [00:03<?, ?batch/s]\n",
      "  File \"/usr/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/spawn.py\", line 125, in _main\n",
      "    prepare(preparation_data)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/spawn.py\", line 236, in prepare\n",
      "Aborted!\n",
      "    _fixup_main_from_path(data['init_main_from_path'])\n",
      "  File \"/usr/lib/python3.10/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n",
      "    main_content = runpy.run_path(main_path,\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 289, in run_path\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7cc7664967a0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    return _run_module_code(code, init_globals, run_name,\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 96, in _run_module_code\n",
      "    _run_code(code, mod_globals, init_globals,\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/content/edm/fid.py\", line 16, in <module>\n",
      "    import torch\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2097, in <module>\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1562, in _shutdown_workers\n",
      "    import torch.nn.intrinsic\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/intrinsic/__init__.py\", line 19, in <module>\n",
      "    if self._persistent_workers or self._workers_status[worker_id]:\n",
      "AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute '_workers_status'\n",
      "    from torch.nn.intrinsic import modules, qat, quantized  # noqa: F401\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/intrinsic/quantized/__init__.py\", line 3, in <module>\n",
      "    from torch.nn.intrinsic.quantized import dynamic, modules  # noqa: F401\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/intrinsic/quantized/dynamic/__init__.py\", line 1, in <module>\n",
      "    from torch.nn.intrinsic.quantized.dynamic.modules import *  # noqa: F403\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/intrinsic/quantized/dynamic/modules/__init__.py\", line 1, in <module>\n",
      "    from torch.nn.intrinsic.quantized.dynamic.modules.linear_relu import LinearReLU\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/intrinsic/quantized/dynamic/modules/linear_relu.py\", line 1, in <module>\n",
      "    from torch.ao.nn.intrinsic.quantized.dynamic import LinearReLU\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/ao/nn/intrinsic/quantized/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/ao/nn/intrinsic/quantized/modules/__init__.py\", line 1, in <module>\n",
      "    from .bn_relu import BNReLU2d, BNReLU3d\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/ao/nn/intrinsic/quantized/modules/bn_relu.py\", line 6, in <module>\n",
      "    import torch.ao.nn.quantized as nnq\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/ao/nn/quantized/__init__.py\", line 1, in <module>\n",
      "    from . import functional\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/ao/nn/quantized/functional.py\", line 11, in <module>\n",
      "    from .modules.utils import _pair_from_first\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/ao/nn/quantized/modules/__init__.py\", line 9, in <module>\n",
      "    import torch.ao.nn.quantizable\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/ao/nn/quantizable/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/ao/nn/quantizable/modules/__init__.py\", line 2, in <module>\n",
      "    from .rnn import LSTM, LSTMCell\n",
      "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 879, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 975, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1072, in get_data\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/torchrun\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 919, in main\n",
      "    run(args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 910, in run\n",
      "    elastic_launch(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 260, in launch_agent\n",
      "    result = agent.run()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/metrics/api.py\", line 137, in wrapper\n",
      "    result = f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/agent/server/api.py\", line 696, in run\n",
      "    result = self._invoke_run(role)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/agent/server/api.py\", line 855, in _invoke_run\n",
      "    time.sleep(monitor_interval)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/api.py\", line 84, in _terminate_process_handler\n",
      "    raise SignalException(f\"Process {os.getpid()} got signal: {sigval}\", sigval=sigval)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/api.py\", line 84, in _terminate_process_handler\n",
      "    raise SignalException(f\"Process {os.getpid()} got signal: {sigval}\", sigval=sigval)\n",
      "torch.distributed.elastic.multiprocessing.api.SignalException: Process 51111 got signal: 2\n"
     ]
    }
   ],
   "source": [
    "#FID for steps=40, solver=Heun\n",
    "!torchrun --standalone --nproc_per_node=1 edm/generate.py --outdir=fid-tmp --seeds=100000-149999 --subdirs --steps=40\\\n",
    "    --network=/content/training-runs/00000-mnist_dataset_downsampled-cond-ddpmpp-edm-gpus1-batch256-fp16/network-snapshot-006000.pkl\n",
    "!torchrun --standalone --nproc_per_node=1 edm/fid.py calc --images=fid-tmp \\\n",
    "    --ref=fid-refs/MNISTdataset.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4HXkjw---bS7",
    "outputId": "d46b8a13-2b7c-43ad-f454-0ff583ad10f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading network from \"/content/training-runs/00000-mnist_dataset_downsampled-cond-ddpmpp-edm-gpus1-batch256-fp16/network-snapshot-006000.pkl\"...\n",
      "[rank0]:[W1210 19:26:34.026106237 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n",
      "Generating 50000 images to \"fid-tmp\"...\n",
      "100% 782/782 [08:50<00:00,  1.47batch/s]\n",
      "Done.\n",
      "Loading dataset reference statistics from \"fid-refs/MNISTdataset.npz\"...\n",
      "Loading Inception-v3 model...\n",
      "Loading images from \"fid-tmp\"...\n",
      "[rank0]:[W1210 19:35:32.391971389 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n",
      "Calculating statistics for 50000 images...\n",
      "100% 782/782 [00:32<00:00, 24.27batch/s]\n",
      "Calculating FID...\n",
      "10.2189\n"
     ]
    }
   ],
   "source": [
    "#FID for steps=18, solver=Euler\n",
    "!torchrun --standalone --nproc_per_node=1 edm/generate.py --outdir=fid-tmp --seeds=100000-149999 --subdirs --solver='euler'\\\n",
    "    --network=/content/training-runs/00000-mnist_dataset_downsampled-cond-ddpmpp-edm-gpus1-batch256-fp16/network-snapshot-006000.pkl\n",
    "!torchrun --standalone --nproc_per_node=1 edm/fid.py calc --images=fid-tmp \\\n",
    "    --ref=fid-refs/MNISTdataset.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OR7UIQXD-bsS",
    "outputId": "15d9d4ed-a081-47c4-bba6-2fa5352a7dcf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading network from \"/content/training-runs/00000-mnist_dataset_downsampled-cond-ddpmpp-edm-gpus1-batch256-fp16/network-snapshot-006000.pkl\"...\n",
      "[rank0]:[W1210 19:43:12.365684946 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n",
      "Generating 50000 images to \"fid-tmp\"...\n",
      "100% 782/782 [19:24<00:00,  1.49s/batch]\n",
      "Done.\n",
      "Loading dataset reference statistics from \"fid-refs/MNISTdataset.npz\"...\n",
      "Loading Inception-v3 model...\n",
      "Loading images from \"fid-tmp\"...\n",
      "[rank0]:[W1210 20:02:44.319597106 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n",
      "Calculating statistics for 50000 images...\n",
      "  1% 7/782 [00:06<12:50,  1.01batch/s]\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \"/content/edm/fid.py\", line 163, in <module>\n",
      "[rank0]:     main()\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1157, in __call__\n",
      "[rank0]:     return self.main(*args, **kwargs)\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1078, in main\n",
      "[rank0]:     rv = self.invoke(ctx)\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1688, in invoke\n",
      "[rank0]:     return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1434, in invoke\n",
      "[rank0]:     return ctx.invoke(self.callback, **ctx.params)\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 783, in invoke\n",
      "[rank0]:     return __callback(*args, **kwargs)\n",
      "[rank0]:   File \"/content/edm/fid.py\", line 131, in calc\n",
      "[rank0]:     mu, sigma = calculate_inception_stats(image_path=image_path, num_expected=num_expected, seed=seed, max_batch_size=batch)\n",
      "[rank0]:   File \"/content/edm/fid.py\", line 62, in calculate_inception_stats\n",
      "[rank0]:     for images, _labels in tqdm.tqdm(data_loader, unit='batch', disable=(dist.get_rank() != 0)):\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/tqdm/std.py\", line 1181, in __iter__\n",
      "[rank0]:     for obj in iterable:\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 701, in __next__\n",
      "[rank0]:     data = self._next_data()\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1465, in _next_data\n",
      "[rank0]:     return self._process_data(data)\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1491, in _process_data\n",
      "[rank0]:     data.reraise()\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils.py\", line 715, in reraise\n",
      "[rank0]:     raise exception\n",
      "[rank0]: AssertionError: Caught AssertionError in DataLoader worker process 1.\n",
      "[rank0]: Original Traceback (most recent call last):\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n",
      "[rank0]:     data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n",
      "[rank0]:     data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n",
      "[rank0]:     data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "[rank0]:   File \"/content/edm/training/dataset.py\", line 98, in __getitem__\n",
      "[rank0]:     assert list(image.shape) == self.image_shape\n",
      "[rank0]: AssertionError\n",
      "\n",
      "E1210 20:02:52.180000 61097 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 61119) of binary: /usr/bin/python3\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/torchrun\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 919, in main\n",
      "    run(args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 910, in run\n",
      "    elastic_launch(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 269, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "edm/fid.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2024-12-10_20:02:52\n",
      "  host      : 159ac9015b3f\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 61119)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "#FID for steps=40, solver=Euler\n",
    "!torchrun --standalone --nproc_per_node=1 edm/generate.py --outdir=fid-tmp --seeds=100000-149999 --subdirs --steps=40 --solver='euler'\\\n",
    "    --network=/content/training-runs/00000-mnist_dataset_downsampled-cond-ddpmpp-edm-gpus1-batch256-fp16/network-snapshot-006000.pkl\n",
    "!torchrun --standalone --nproc_per_node=1 edm/fid.py calc --images=fid-tmp \\\n",
    "    --ref=fid-refs/MNISTdataset.npz    "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
